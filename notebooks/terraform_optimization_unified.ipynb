{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unified Terraform Optimization Pipeline\n",
    "\n",
    "This notebook combines all our optimization experiments with advanced monitoring and orchestration:\n",
    "\n",
    "1. MLflow Integration:\n",
    "   - Experiment tracking\n",
    "   - Model versioning\n",
    "   - Hyperparameter optimization\n",
    "   - Artifact management\n",
    "\n",
    "2. KubeFlow Integration:\n",
    "   - Pipeline orchestration\n",
    "   - Distributed training\n",
    "   - Resource management\n",
    "   - Model serving\n",
    "\n",
    "3. LangFuse Integration:\n",
    "   - Inference monitoring\n",
    "   - Cost tracking\n",
    "   - Performance analysis\n",
    "   - Quality metrics\n",
    "\n",
    "4. Advanced Features:\n",
    "   - Multi-task optimization\n",
    "   - Continuous fine-tuning\n",
    "   - PR generation\n",
    "   - A/B testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import mlflow\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "from kfp.components import create_component_from_func\n",
    "import pytorch_lightning as pl\n",
    "from langfuse import Langfuse\n",
    "import optuna\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Setup monitoring\n",
    "from model_training.monitoring import MonitoringCallback, InferenceMonitor\n",
    "\n",
    "# Initialize platforms\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(\"terraform_unified_optimization\")\n",
    "client = kfp.Client(host=\"http://localhost:8000\")\n",
    "\n",
    "# Load config\n",
    "config = {}\n",
    "with open(\"/workspace/configs/api_keys.env\") as f:\n",
    "    for line in f:\n",
    "        if '=' in line:\n",
    "            key, value = line.strip().split('=', 1)\n",
    "            config[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Optimization Targets\n",
    "\n",
    "Set up different optimization scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "@dataclass\n",
    "class OptimizationTarget:\n",
    "    name: str\n",
    "    resource_types: List[str]\n",
    "    metrics: List[str]\n",
    "    constraints: Dict[str, any]\n",
    "    success_criteria: Dict[str, float]\n",
    "\n",
    "TARGETS = {\n",
    "    \"cost\": OptimizationTarget(\n",
    "        name=\"cost_optimization\",\n",
    "        resource_types=[\"aws_instance\", \"aws_eks_cluster\", \"aws_lambda_function\"],\n",
    "        metrics=[\"cost\", \"performance\", \"availability\"],\n",
    "        constraints={\n",
    "            \"min_availability\": 0.99,\n",
    "            \"max_response_time\": 200\n",
    "        },\n",
    "        success_criteria={\n",
    "            \"cost_reduction\": 0.10,\n",
    "            \"performance_degradation_max\": 0.05\n",
    "        }\n",
    "    ),\n",
    "    \"security\": OptimizationTarget(\n",
    "        name=\"security_optimization\",\n",
    "        resource_types=[\"aws_security_group\", \"aws_iam_role\", \"aws_kms_key\"],\n",
    "        metrics=[\"compliance_score\", \"attack_surface\", \"cost\"],\n",
    "        constraints={\n",
    "            \"min_compliance\": 0.95,\n",
    "            \"max_cost_increase\": 0.15\n",
    "        },\n",
    "        success_criteria={\n",
    "            \"compliance_improvement\": 0.20,\n",
    "            \"attack_surface_reduction\": 0.30\n",
    "        }\n",
    "    ),\n",
    "    \"performance\": OptimizationTarget(\n",
    "        name=\"performance_optimization\",\n",
    "        resource_types=[\"aws_rds_instance\", \"aws_elasticache_cluster\", \"aws_elasticsearch_domain\"],\n",
    "        metrics=[\"latency\", \"throughput\", \"cost\"],\n",
    "        constraints={\n",
    "            \"max_cost_increase\": 0.20,\n",
    "            \"min_availability\": 0.995\n",
    "        },\n",
    "        success_criteria={\n",
    "            \"latency_reduction\": 0.25,\n",
    "            \"throughput_improvement\": 0.15\n",
    "        }\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Pipeline Components\n",
    "\n",
    "Enhanced data processing with KubeFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def process_terraform_data(data_path: str, target: str) -> Dict[str, str]:\n",
    "    \"\"\"Process Terraform dataset with target-specific filtering\"\"\"\n",
    "    import sqlite3\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Load TerraDS database\n",
    "    conn = sqlite3.connect(data_path)\n",
    "    \n",
    "    # Get target configuration\n",
    "    target_config = TARGETS[target]\n",
    "    \n",
    "    # Extract relevant resources\n",
    "    resource_types = \"','\".join(target_config.resource_types)\n",
    "    df = pd.read_sql(f\"\"\"\n",
    "        SELECT r.*, m.repository_id \n",
    "        FROM resources r \n",
    "        JOIN modules m ON r.module_id = m.id\n",
    "        WHERE r.is_managed = 1\n",
    "        AND r.type IN ('{resource_types}')\n",
    "    \"\"\", conn)\n",
    "    \n",
    "    # Save processed data\n",
    "    output_dir = Path(\"/tmp/processed_data\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Split into train/validation\n",
    "    train_df = df.sample(frac=0.8, random_state=42)\n",
    "    val_df = df.drop(train_df.index)\n",
    "    \n",
    "    # Save datasets\n",
    "    train_path = output_dir / f\"{target}_train.parquet\"\n",
    "    val_path = output_dir / f\"{target}_val.parquet\"\n",
    "    \n",
    "    train_df.to_parquet(train_path)\n",
    "    val_df.to_parquet(val_path)\n",
    "    \n",
    "    return {\n",
    "        \"train\": str(train_path),\n",
    "        \"validation\": str(val_path)\n",
    "    }\n",
    "\n",
    "# Create KubeFlow component\n",
    "process_data_op = create_component_from_func(\n",
    "    func=process_terraform_data,\n",
    "    base_image='python:3.10',\n",
    "    packages_to_install=['pandas', 'pyarrow', 'sqlite3']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Training Pipeline\n",
    "\n",
    "Multi-task training with MLflow tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def train_model(\n",
    "    data_paths: Dict[str, str],\n",
    "    target: str,\n",
    "    hyperparams: Dict\n",
    ") -> str:\n",
    "    \"\"\"Train model with comprehensive monitoring\"\"\"\n",
    "    import mlflow\n",
    "    from model_training.continuous_finetuning import TerraformMultiTaskOptimizer\n",
    "    \n",
    "    # Initialize monitoring\n",
    "    monitor = MonitoringCallback(\n",
    "        langfuse_public_key=config[\"LANGFUSE_PUBLIC_KEY\"],\n",
    "        langfuse_secret_key=config[\"LANGFUSE_SECRET_KEY\"]\n",
    "    )\n",
    "    \n",
    "    with mlflow.start_run() as run:\n",
    "        # Log configuration\n",
    "        mlflow.log_params({\n",
    "            **hyperparams,\n",
    "            \"target\": target,\n",
    "            \"resource_types\": TARGETS[target].resource_types\n",
    "        })\n",
    "        \n",
    "        # Initialize model\n",
    "        model = TerraformMultiTaskOptimizer(\n",
    "            model_name=hyperparams[\"model_name\"],\n",
    "            targets=TARGETS,\n",
    "            learning_rate=hyperparams[\"learning_rate\"]\n",
    "        )\n",
    "        \n",
    "        # Configure trainer\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=hyperparams[\"epochs\"],\n",
    "            accelerator=\"gpu\",\n",
    "            devices=1,\n",
    "            precision=16,\n",
    "            callbacks=[monitor],\n",
    "            accumulate_grad_batches=hyperparams.get(\"grad_accum\", 4)\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        trainer.fit(\n",
    "            model,\n",
    "            train_dataloaders=create_dataloader(data_paths[\"train\"]),\n",
    "            val_dataloaders=create_dataloader(data_paths[\"validation\"])\n",
    "        )\n",
    "        \n",
    "        # Log final metrics\n",
    "        metrics = trainer.callback_metrics\n",
    "        mlflow.log_metrics({\n",
    "            k: v.item() for k, v in metrics.items()\n",
    "        })\n",
    "        \n",
    "        # Save model\n",
    "        mlflow.pytorch.log_model(model, \"model\")\n",
    "        \n",
    "        return run.info.run_id\n",
    "\n",
    "# Create KubeFlow component\n",
    "train_model_op = create_component_from_func(\n",
    "    func=train_model,\n",
    "    base_image='pytorch/pytorch:2.2.0-cuda12.1-cudnn8-runtime',\n",
    "    packages_to_install=['mlflow', 'pytorch-lightning', 'langfuse']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation and PR Generation\n",
    "\n",
    "Advanced evaluation with multiple criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def evaluate_and_generate_pr(\n",
    "    run_id: str,\n",
    "    target: str,\n",
    "    eval_data: str\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"Evaluate model and generate PR if successful\"\"\"\n",
    "    import mlflow\n",
    "    from model_training.monitoring import InferenceMonitor\n",
    "    \n",
    "    # Initialize monitoring\n",
    "    monitor = InferenceMonitor(\n",
    "        langfuse_public_key=config[\"LANGFUSE_PUBLIC_KEY\"],\n",
    "        langfuse_secret_key=config[\"LANGFUSE_SECRET_KEY\"],\n",
    "        model_version=run_id\n",
    "    )\n",
    "    \n",
    "    # Start evaluation trace\n",
    "    trace = monitor.start_optimization_trace(\n",
    "        resource_type=TARGETS[target].resource_types[0],\n",
    "        optimization_target=target\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Load model\n",
    "        model = mlflow.pytorch.load_model(f\"runs:/{run_id}/model\")\n",
    "        \n",
    "        # Evaluate model\n",
    "        metrics = evaluate_model(model, eval_data)\n",
    "        \n",
    "        # Log evaluation metrics\n",
    "        with mlflow.start_run(run_id=run_id):\n",
    "            mlflow.log_metrics(metrics)\n",
    "        \n",
    "        # Check success criteria\n",
    "        success = all(\n",
    "            metrics[metric] >= threshold\n",
    "            for metric, threshold in TARGETS[target].success_criteria.items()\n",
    "        )\n",
    "        \n",
    "        if success:\n",
    "            # Register model\n",
    "            mv = mlflow.register_model(\n",
    "                f\"runs:/{run_id}/model\",\n",
    "                f\"terraform-optimizer-{target}\"\n",
    "            )\n",
    "            \n",
    "            # Generate PR\n",
    "            pr_info = generate_optimization_pr(\n",
    "                model=model,\n",
    "                target=target,\n",
    "                metrics=metrics\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"model_version\": mv.version,\n",
    "                \"pr_url\": pr_info[\"pr_url\"],\n",
    "                \"improvements\": str(pr_info[\"improvements\"])\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            \"model_version\": \"\",\n",
    "            \"pr_url\": \"\",\n",
    "            \"improvements\": str(metrics)\n",
    "        }\n",
    "        \n",
    "    finally:\n",
    "        trace.end()\n",
    "\n",
    "# Create KubeFlow component\n",
    "evaluate_op = create_component_from_func(\n",
    "    func=evaluate_and_generate_pr,\n",
    "    base_image='pytorch/pytorch:2.2.0-cuda12.1-cudnn8-runtime',\n",
    "    packages_to_install=['mlflow', 'langfuse']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Unified Pipeline Definition\n",
    "\n",
    "Complete pipeline with all components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "@dsl.pipeline(\n",
    "    name='terraform-unified-pipeline',\n",
    "    description='End-to-end Terraform optimization pipeline'\n",
    ")\n",
    "def optimization_pipeline(\n",
    "    data_path: str = \"/network/mlops/datasets/TerraDS.sqlite\",\n",
    "    target: str = \"cost\",\n",
    "    hyperparams: Dict = {\n",
    "        \"model_name\": \"nuibang/Cline_FuseO1-DeepSeekR1-Qwen2.5-Coder-32B-Preview\",\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"epochs\": 10,\n",
    "        \"grad_accum\": 4\n",
    "    }\n",
    "):\n",
    "    # Process data\n",
    "    with dsl.ParallelFor(list(TARGETS.keys())) as target:\n",
    "        process_data = process_data_op(\n",
    "            data_path=data_path,\n",
    "            target=target\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        train = train_model_op(\n",
    "            data_paths=process_data.output,\n",
    "            target=target,\n",
    "            hyperparams=hyperparams\n",
    "        )\n",
    "        \n",
    "        # Evaluate and generate PR\n",
    "        evaluate = evaluate_op(\n",
    "            run_id=train.output,\n",
    "            target=target,\n",
    "            eval_data=process_data.outputs[\"validation\"]\n",
    "        )\n",
    "        \n",
    "        # Deploy if evaluation successful\n",
    "        with dsl.Condition(evaluate.outputs[\"model_version\"] != \"\"):\n",
    "            deploy_op = kfp.dsl.ContainerOp(\n",
    "                name='deploy-model',\n",
    "                image='kfserving-deployer:latest',\n",
    "                command=['python', 'deploy.py'],\n",
    "                arguments=[\n",
    "                    '--model-name', f'terraform-optimizer-{target}',\n",
    "                    '--version', evaluate.outputs[\"model_version\"]\n",
    "                ]\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Pipeline\n",
    "\n",
    "Execute the complete pipeline with monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Submit pipeline\n",
    "run = client.create_run_from_pipeline_func(\n",
    "    optimization_pipeline,\n",
    "    arguments={\n",
    "        'data_path': \"/network/mlops/datasets/TerraDS.sqlite\",\n",
    "        'target': \"cost\",\n",
    "        'hyperparams': {\n",
    "            \"model_name\": \"nuibang/Cline_FuseO1-DeepSeekR1-Qwen2.5-Coder-32B-Preview\",\n",
    "            \"learning_rate\": 2e-5,\n",
    "            \"epochs\": 10,\n",
    "            \"grad_accum\": 4\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Print monitoring URLs\n",
    "print(f\"Pipeline run: {run.run_id}\")\n",
    "print(f\"MLflow UI: http://localhost:5000\")\n",
    "print(f\"KubeFlow UI: http://localhost:8000\")\n",
    "print(f\"LangFuse UI: http://localhost:3000\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}